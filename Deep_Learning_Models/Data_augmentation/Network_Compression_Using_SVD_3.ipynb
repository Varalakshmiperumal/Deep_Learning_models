{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "JiR9voy_ezZj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RCIWLqbzLif"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3zRwPbKcQqM",
        "outputId": "ff0a4324-4b56-4e88-dc2f-3fd4bb973138"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9L20-HEzRpH"
      },
      "source": [
        "# Preprocessing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "R_uvB9kCcQ6d"
      },
      "outputs": [],
      "source": [
        "# normalise the data  to 0-1\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# flatten the data\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "# convert the output to categorical\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Iron5FBbnd"
      },
      "source": [
        "# Initializing the baseline model\n",
        "\n",
        "We are initializing the baseline model and then tune it with SVD with singular value 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLJOUOivBZ0r",
        "outputId": "c2f421da-71e9-484a-bbfe-40430addd593"
      },
      "outputs": [],
      "source": [
        "input_dim = 784\n",
        "num_classes = 10\n",
        "\n",
        "baseline = models.Sequential([\n",
        "    layers.Input(shape=(784,)),\n",
        "\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "baseline.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "xFPmTyZ8fD8P"
      },
      "outputs": [],
      "source": [
        "# Extract weights for fine-tuning\n",
        "W1, b1 = baseline.layers[0].get_weights()\n",
        "W2, b2 = baseline.layers[15].get_weights()\n",
        "\n",
        "# Initialize trainable tensors\n",
        "W1_var = tf.Variable(W1, dtype=tf.float32)\n",
        "b1_var = tf.Variable(b1, dtype=tf.float32)\n",
        "W2_var = tf.Variable(W2, dtype=tf.float32)\n",
        "b2_var = tf.Variable(b2, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accuray of untrained base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGr6tMcqe69m",
        "outputId": "3fa686f5-fbb7-4400-8b05-eaa47aedf233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 2.3053\n",
            "Test Accuracy: 0.1003\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = baseline.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "4IzXUF40Mlyq"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "batch_size = 128\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Finetuning the basemodel using svd in every epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  feedforward pass always uses\n",
        "$$ {W}^{(l)} = {U}^{(l)}_{:,1:20} {S}^{(l)}_{1:20,1:20} {{V}^{(l)}}^{‚ä§}_{:,1:20}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0CYcUO6e7A8",
        "outputId": "2af62627-58b5-4320-d936-ac809d7c6882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch_____:1/10\n",
            "Step 0: Loss = 2.3119\n",
            "Step 100: Loss = 0.3634\n",
            "Step 200: Loss = 0.2867\n",
            "Step 300: Loss = 0.2310\n",
            "Step 400: Loss = 0.1545\n",
            "\n",
            "Epoch_____:2/10\n",
            "Step 0: Loss = 0.1727\n",
            "Step 100: Loss = 0.1914\n",
            "Step 200: Loss = 0.0406\n",
            "Step 300: Loss = 0.0942\n",
            "Step 400: Loss = 0.0894\n",
            "\n",
            "Epoch_____:3/10\n",
            "Step 0: Loss = 0.0949\n",
            "Step 100: Loss = 0.1395\n",
            "Step 200: Loss = 0.0840\n",
            "Step 300: Loss = 0.0356\n",
            "Step 400: Loss = 0.0748\n",
            "\n",
            "Epoch_____:4/10\n",
            "Step 0: Loss = 0.0734\n",
            "Step 100: Loss = 0.0322\n",
            "Step 200: Loss = 0.0933\n",
            "Step 300: Loss = 0.0929\n",
            "Step 400: Loss = 0.0222\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 19:59:41.876079: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch_____:5/10\n",
            "Step 0: Loss = 0.1119\n",
            "Step 100: Loss = 0.0646\n",
            "Step 200: Loss = 0.0945\n",
            "Step 300: Loss = 0.0555\n",
            "Step 400: Loss = 0.0334\n",
            "\n",
            "Epoch_____:6/10\n",
            "Step 0: Loss = 0.0393\n",
            "Step 100: Loss = 0.0639\n",
            "Step 200: Loss = 0.0624\n",
            "Step 300: Loss = 0.0926\n",
            "Step 400: Loss = 0.0317\n",
            "\n",
            "Epoch_____:7/10\n",
            "Step 0: Loss = 0.0172\n",
            "Step 100: Loss = 0.0244\n",
            "Step 200: Loss = 0.0696\n",
            "Step 300: Loss = 0.0176\n",
            "Step 400: Loss = 0.0644\n",
            "\n",
            "Epoch_____:8/10\n",
            "Step 0: Loss = 0.0338\n",
            "Step 100: Loss = 0.0385\n",
            "Step 200: Loss = 0.0427\n",
            "Step 300: Loss = 0.0689\n",
            "Step 400: Loss = 0.0158\n",
            "\n",
            "Epoch_____:9/10\n",
            "Step 0: Loss = 0.0325\n",
            "Step 100: Loss = 0.0140\n",
            "Step 200: Loss = 0.0028\n",
            "Step 300: Loss = 0.0053\n",
            "Step 400: Loss = 0.0171\n",
            "\n",
            "Epoch_____:10/10\n",
            "Step 0: Loss = 0.0082\n",
            "Step 100: Loss = 0.0171\n",
            "Step 200: Loss = 0.0570\n",
            "Step 300: Loss = 0.0135\n",
            "Step 400: Loss = 0.0314\n",
            "\n",
            "Final Test Accuracy (with D=20): 97.68%\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.001  \n",
        "D = 20 \n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_ds = train_ds.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch_____:{epoch+1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_ds):\n",
        "        with tf.GradientTape() as tape:\n",
        "            s1, u1, v1 = tf.linalg.svd(W1_var, full_matrices=False)\n",
        "            W1_svd = tf.matmul(u1[:, :D], tf.matmul(tf.linalg.diag(s1[:D]), tf.transpose(v1[:, :D])))\n",
        "            s2, u2, v2 = tf.linalg.svd(W2_var, full_matrices=False)\n",
        "            W2_svd = tf.matmul(u2[:, :D], tf.matmul(tf.linalg.diag(s2[:D]), tf.transpose(v2[:, :D])))\n",
        "            x1 = tf.nn.relu(tf.matmul(tf.cast(x_batch, tf.float32), W1_svd) + b1_var)\n",
        "            logits = tf.matmul(x1, W2_svd) + b2_var\n",
        "            loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_batch, tf.nn.softmax(logits))) \n",
        "\n",
        "        grads = tape.gradient(loss, [W1_var, b1_var, W2_var, b2_var])\n",
        "        optimizer.apply_gradients(zip(grads, [W1_var, b1_var, W2_var, b2_var]))\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}: Loss = {loss.numpy():.4f}\")\n",
        "\n",
        "s1, u1, v1 = tf.linalg.svd(W1_var, full_matrices=False)\n",
        "W1_svd = tf.matmul(u1[:, :D], tf.matmul(tf.linalg.diag(s1[:D]), tf.transpose(v1[:, :D])))\n",
        "s2, u2, v2 = tf.linalg.svd(W2_var, full_matrices=False)\n",
        "W2_svd = tf.matmul(u2[:, :D], tf.matmul(tf.linalg.diag(s2[:D]), tf.transpose(v2[:, :D])))\n",
        "\n",
        "\n",
        "x1_test = tf.nn.relu(tf.matmul(tf.cast(x_test, tf.float32), W1_svd) + b1_var)\n",
        "logits_test = tf.matmul(x1_test, W2_svd) + b2_var\n",
        "preds = tf.argmax(tf.nn.softmax(logits_test), axis=1)\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(preds, tf.argmax(y_test, axis=1)), tf.float32)) \n",
        "\n",
        "print(f\"\\nFinal Test Accuracy (with D={D}): {acc.numpy() * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
